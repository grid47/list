### Problem Statement
You are given an integer array `arr`. A mountain is defined as a subarray that meets the following criteria:
- It has at least three elements.
- There exists an index `i` (0 < i < arr.length - 1) such that:
  - arr[i-1] < arr[i] > arr[i+1].
Your task is to find the length of the longest mountain in the array.

### Approach
1. **Initial Check**: If the array has fewer than 3 elements, return 0 as it can't form a mountain.

2. **Prefix and Suffix Arrays**:
   - Create two arrays, `pre` and `suf`, of the same length as `arr` to store the counts of increasing and decreasing sequences:
     - `pre[i]` counts how many elements before index `i` are part of an increasing sequence.
     - `suf[i]` counts how many elements after index `i` are part of a decreasing sequence.

3. **Fill Prefix Array**:
   - Traverse the array from left to right:
     - If the current element is greater than the previous element, increment the current count; otherwise, reset it to zero.

4. **Fill Suffix Array**:
   - Traverse the array from right to left:
     - If the current element is greater than the next element, increment the current count; otherwise, reset it to zero.

5. **Calculate Maximum Mountain Length**:
   - Iterate through the array and check for potential mountain peaks:
     - A peak exists if both `pre[i]` and `suf[i]` are greater than 0. The length of the mountain is `pre[i] + suf[i] + 1`.
   - Update the maximum length found.

6. **Final Check**: If the longest mountain length is 1 (indicating no mountain was found), return 0.

### Complexity
- **Time Complexity**: O(N), where N is the length of the input array. Each element is processed a constant number of times.
- **Space Complexity**: O(N) for the `pre` and `suf` arrays.
